%%
%% This is file `sample-sigchi.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigchi')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigchi.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigchi]{acmart}
%\usepackage [slovene]{babel}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage[utf8]{inputenc}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-9999-9/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Sound 2121: The Future of Music is Natural}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

%\author{Jordan Deja}
%\email{jordan.deja@famnit.upr.si}
%\affiliation{%
%  \institution{University of Primorska\\UP FAMNIT}
%  \streetaddress{Glagoljaška 8}
%  \city{Koper}
%  \state{Slovenia}
%  \postcode{SI-6000}
%}

%\author{Nuwan Attygale}
%\email{nuwan.attygalle@upr.si}
%\affiliation{%
%  \institution{University of Primorska\\UP FAMNIT}
%  \streetaddress{Glagoljaška 8}
%  \city{Koper}
%  \state{Slovenia}
%  \postcode{SI-6000}
%}

%\author{Klen Čopič Pucihar}
%%\authornotemark[1]
%\email{klen.copic@famnit.upr.si}
%\affiliation{%
%  \institution{University of Primorska\\UP FAMNIT}
%  \streetaddress{Glagoljaška 8}
%  \city{Koper}
%  \state{Slovenia}
%  \postcode{SI-6000}
%}
%\affiliation{%
%  \institution{Fakulteta za Informacijske Študije}
%  \streetaddress{Glagoljaška 8}
%  \city{Novo mesto}
%  \state{Slovenija}
%  \postcode{SI-6000}
%}

%\author{Matjaž Kljun}
%\email{matjaz.kljun@famnit.upr.si}
%\orcid{1234-5678-9012}
%\affiliation{%
%  \institution{University of Primorska\\UP FAMNIT}
%  \streetaddress{Glagoljaška 8}
%  \city{Koper}
%  \state{Slovenia}
%  \postcode{SI-6000}
%}
%\affiliation{%
%  \institution{Fakulteta za Informacijske Študije}
%  \streetaddress{Glagoljaška 8}
%  \city{Novo mesto}
%  \state{Slovenija}
%  \postcode{SI-6000}
%}

\settopmatter{printacmref=false}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Music has always been an integral part of our society since the prehistoric times. For the past five centuries, music instruments have been perfected and the industry is nowadays worth billions of dollars. With recent innovations in computer interfaces, information retrieval and artificial intelligence, playing music is not in the sole domain of humans anymore. Thus we are faced with the questions: ``What really is music? What is the future of music? How will we consume music a hundred years from now?'' In this paper, we shortly present how music has been consumed throughout history how we imagine it a century from now. We make a wild speculation about the future of music and its interface, while encouraging the discussion regarding these visions. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003123.10011758</concept_id>
       <concept_desc>Human-centered computing~Interaction design theory, concepts and paradigms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Interaction design theory, concepts and paradigms}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{music, interface, interaction design, sound}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{acmart-master-2/samples/test2bw.png}
  \caption{Concept: We see a future where we no longer need tangible interfaces. Rather humans would let go of these interfaces to give way to a more natural and seamless music interface.}
  \Description{photo of a user removing headphones.}
  \label{fig:teaser}
\end{teaserfigure}
%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
%Humans have been listening to music since 

Music is considered to be culturally universal \cite{campbell1997music, seeger1971reflections} and present across all parts of the globe, reshaping the ways human live, express themselves and convey emotions  \cite{juslin2001music,montagu2017music}. Humans have been expressing themselves through music for a very long time. Music as well as art has helped humans in terms of survival, expressing their emotions, forging a sense of group identity and mutual trust, which as a consequence enabled them to become so successful~\cite{conard2009new}. 


Similar to speech, listening to music can bring us to a state full of emotions as changes in vocal parameters take place. Emotions are extended by the composer of the piece, in turn, realising concepts such as happiness and sadness. Because of this, music is considered as a popular and easily-applicable means for triggering emotions \cite{kappert2019aim}. 

It is believed that music originated from naturally occurring sounds and rhythms that humans echoed by merging them in patterns, making repetitions and changing tonality. For example, it is believed that ambient sounds  times, humans felt safer when it rains. This is because predators tend to stop hunting for prey when it rains. Fast forward to now, this probably explains why humans would feel safer and more relaxed when listening to ambient rain sounds \cite{aramaki2017bridging}. Evolution has ingrained in our DNA this behaviour and we believe it will continue to be so in the next coming years. 



Our ancestors began making music with the use of their voice box, which is considered as the first musical instrument. Neanderthals and modern humans developed their vocal anatomy capable of singing about a million years ago \cite{montagu2017music}. Then they began to create rhythm by clapping their hands \cite{kassler1987dancing}. Then, they began using the objects around them. They recreated patterns by smacking stones and sticks \cite{montagu2014horns}. Long after, our ancestors worked on improving the craft of designing and building musical instruments throughtout the years. In prehistoric and ancient times, humans first created music by blowing wind pipes which are now known as wind instruments \cite{conard2009female}.



According to scientists, the first species of humans developed their voice box (considered as the first musical instrument) around 530,000 years ago~\cite{,morley2013prehistory}, while the oldest musical instruments found in Europe are the bone flutes that are at least 42 thousand years old \cite{higham2012tauesting,wallin2001origins,conard2009new}. Some authors even argue that since the oldest instruments found are so sophisticated there must have been less sophisticated instruments used by humans before \cite{conard2009new,morley2013prehistory}. Nevertheless, the instruments the humans made and used have rapidly evolved together with the complexity of music compositions in the last couple of centuries. As newer technologies are introduced, more ways of creating, producing and even sharing music \cite{voida2005listening} are also taking place. 

 

 In the classical and renaissance eras, guitars with strings that can be strummed, piano with keys that can be pressed, cymbals made of metal that can be slammed against each other, were used by humans to create sound. The atomic era and the introduction of the silicon led to MIDI interfaces, electric guitars and synthesizers; devices made of circuits that recreate traditional music instruments, and can be connected to the computer. Now, the information era has led to the development of algorithms, information retrieval (MIR) and artificial intelligence (AI) techniques that allowed humans to work with computers in creating new content. This also introduced social platforms that allowed humans to share and appreciate musical content. As more technological innovations are introduced, humans have also evolved on how and why they interact with music. 



This paper attempts to share the authors' visions of how humans create, share and interact with music in the future eras. We present our position based on the trends on (1) how musical instruments have evolved, (2) how humans have used them and (3) how can we go back to a more natural but technologically-empowered musical experiences? These visions were also derived from shared ideas in limited crowd-sourcing activities we did. We present 3 distinct scenarios where our natural musical interface can be best applied. Lastly, we present questions and challenges that provoke discussions involving usability, security, intellectual property and many other relevant key topics on music. 
% styles and types of music 
% instruments
% recent innovations in music, IR, AI 

%\section{Background}
%paragraph 1: Humans instint of safety and the role of music 
%paragraph 2: trends of how music instruments have evolved

\section{Re-imagining music and the music interface}
% paragraph 1: music an experience that you can be immersed in without losing touch of reality
Humans create and consume music based on four obvious purposes: (1) dancing as a social exercise, (2) providing a common form of personal or community entertainment, (3) communicating our ideas and emotions and (4) having and celebrating rituals and other activities \cite{montagu2017music}. The visions we present are anchored on these purposes and are aligned with the technological innovations involving musical instruments. Over the years, recent innovations have attempted to address open problems in music. We use AI techniques to help humans compose music [semi-]automatically; use MIR algorithms to help composers express their emotions and use AR to make immersive music experiences. We envision the future of a music interface that combines all these innovations into a natural interface. We present two ways on how we re-imagined them: 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{acmart-master-2/samples/thinkbnw.png}
  \caption{Concept: Humans do not need wide boards, mobile devices, or digital walls. Ideas and concepts are conceived mentally and are interpreted into \textit{"reality"} as musical or creative instructions by algorithms present in a global neurological interface. }
  \Description{Figure of a person thinking and not touching anything.}
  \label{fig: think}
\end{figure}

\textit{Music not all about sound and can be a naturally-immersive experience without losing touch of reality}. Many people believe that the future will be a world where distinguishing reality from the virtual would be challenging \cite{tamura2001mixed}. However, we believe that the future will be its exact opposite. Society will come to a point where it will halt its transition to going completely-virtual but rather, take a step back and move towards going more natural. After all, music is all about rhythmic vibrations. It is not just about sounds. Even though the deaf struggle with hearing, the part of the brain responsible for hearing works perfectly \cite{abcsciencemusic}. In order to feel music, you do not need to actually hear music but rather receive the signals and vibrations to the hearing region of the brain. Because of this, we envision a future where you do not need external devices (such as headsets or speakers) to be able to hear music, enjoy concerts and many others in a safer and seamless way. Since one of the primary instincts of humans is survival \cite{khantzian1983self}, innovations will push towards sustainability - developing environments and spaces with less carbon footprint but with swifter technological infrastructure. We envision a future where objects will not have brutalist or neo-futuristic designs. Rather, we see a future of natural objects that are connected to the cloud where it has access to superb computing power. It is no secret - microcontrollers are already available and can be plugged into the brain. These interfaces will be out there and will allow us to be connected with other objects \cite{musk2019integrated}. These objects, are our typical things we see in our natural surroundings, but equipped with special nano-chips that allow them to be part of the global link of information; capable of moving themselves depending on the needed use. These chips allow our natural objects to be part of a global network where, humans are a part of through a neurological interface. Thereby allowing humans and objects to telepathically-communicate. Sound signals will be delivered straight into our auditory cortex, into the brain directly. People will no longer have to depend on their ears to listen and hear things. As such, people could enjoy music even while spacewalking, diving, skiing or surfing. People will enjoy music and will help them keep going. 

Looking at the trends of how humans created music:  they pressed two stones against each other to produce basic rhythms and sounds. They began developing instruments with parts, keys and buttons that can be pressed to produce sounds and synths. In any approach, humans transferred their creative thoughts unto the interface of these instruments through a pressing/hitting motion. In the future we envision, humans can feel their emotions and these will be amplified by the music naturally produced by the objects surrounding them. No more pressing, hitting, strumming or the like (see Fig \ref{fig: think}. Algorithms design and produce rhythms in real-time and have them played via vibration by these nearby objects (moving on their own). If humans pressed buttons in the past, humans will simply need to think of these emotions and sounds, and the objects near them will seamlessly produce the vibrations recreating these sounds. Every object in the room can produce a unique rhythm, providing a new definition of audio augmented reality. Humans will get to enjoy their favorite tunes through this natural interface where instructions and processes are communicated through this neurological link online. We believe that through this neurological interface, humans can create and experience music without losing touch of reality. Humans can perform their daily tasks and be able to seamlessly enjoy the music they hear, produced by the objects around them without having to move a single muscle in their bodies. 

\textit{Music as a gateway for human emotions}. Traditionally, there are two ways on how music becomes a gateway for our emotions. If we feel sad, we wish to hear music so we can reflect, dive deeper and understand the sadness that we feel [emotions going in]. This experience gives us lessons on how to manage our emotions, and how to be stronger. At times, we may feel sad so we wish to hear music in order to improve our mood [emotions going out] and spend the better part of our days. Through our natural and seamless interface, humans can create gateways for their emotions with music. As humans are connected to the global highway of information, unobtrusive sensors no longer need to detect and distinguish the current affect that they feel. Because of their neurological connection to the world, their emotions are easily read and \textit{felt} by the objects around them. Similar to how empathetic spaces that are context and emotion aware, objects nearby will be act as local producers of music to either amplify or address the emotions that humans are feeling. 
% gateway cos two way in and out 
%\textit{We create music because of the emotions we wish to express}. Sentence here. 
% music to express/share
% i am heartbroken when i hear some song i can get to understand more what im feeling
% this music blends with my emotion and it amplifies 
% i am a musician i am expressing my feelings with music 
% music comes from emotions and we create music to retain, continue experiencing these emotions
% 
\section{Design Scenarios}
To better explain how we re-imagined this natural music interface, we propose design scenarios. In these events, we describe how our vision is implemented based on different parameters and conditions. First we imagine being able to listen to music while performing wet activities (such as surfing, scuba diving, etc) without the need for waterproof music gear. Second, we imagine humans being able to listen to music in vacuum spaces such as outer space. We also utilize \textit{the sound of planets, stars and galaxies} as objects to create music from their vibrations. Third, we present a scenario where humans can easily augment their emotional experiences using music with the blink of an eye. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{acmart-master-2/samples/surfbnw.png}
  \caption{Concept: Listening to music while surfing in the wide ocean will no longer require waterproof music gear. Rather, natural elements that are interlinked together create vibrations that humans can hear. Humans can finally achieve a non-obtrusive way of listening music while enjoying their wet hobbies. }
  \Description{Figure of a surfer headed to the sea listening to music without the need of special gear.}
  \label{fig: surf}
\end{figure}
% scenario : swimming in corals/alone in the forest. describe tech. add photos
\textit{No need to add background music in watersport videos; you listen to them as you surf}. It is a perfect sunny day. The wind blows nice, and the water is just right. Cuauhtli decides that it is the perfect time to go surfing (as seen in Fig \ref{fig: surf}). He prefers to feel this adrenaline rush with the sound of rock music in his ears. If we were in 2020, Cuauhtli he has to wear water-proof headset while surfing and he has to fit the headset tightly onto his ears to prevent it from falling off. And also when surfing, Cuauhtli has to hear his surroundings perfectly in order to surf properly and not endager his life. In order to do this, he has to balance spatial awareness and enjoy at the same time, which takes a lot of effort \cite{fuchs2018dancing}. But if Cuahtli has to wear a headset, this will be problematic since he will struggle with hearing the background noise. Cuauhtli in 2120 will not have this problem anymore since, not only can he hear his preferred rock music, but he can also hear this as it blends with the sound of the environment around him.  Not only that, if Cuauhtli wants to listen to the environment, the algorithm understands this and can mute the music in full - just through his thoughts. This can be done using two approaches. First, Cuauthli can listen to his favorite track in full sound and when he wants to listen to the background noise, his neurological link will understand this and allow the environment sound to be heard all the way to his brain. Second, the noise of the environment around him can be used as an input and then be processed to create a dynamic track. This will be based on Cuauthli's favorite songs. Algorithms produce a specific tune that fits his current preferences and allows him to not lose his sense as he dives deep into the sea. 

% scenario: algorithms that easily convert our emotion stimuli into actual music
\textit{Creating music from vacuum spaces using the sound of space}. Sentence here. 
%NUWAN also add your stuff here. Like the previous scenario. 

% discovering new music 
\textit{Amplifying emotions at the blink of an eye}. It is a rainy day and Cuauthli is sitting by the window, thinking about his lover. Since he is in Germany for a research visit, he misses his significant other dearly. Cuauthli would love to get lost on his thoughts about her. He then decides to listen to a song which helps him to keep thinking about his lover. The music helps him to draw all the memories because it helps him to concentrate on her, on the memories of her by. This is done by reducing other background noise as inputs and allows him to focus on the memories that are in his brain. And after a short while,  now the rain is over and Cuauthli needs to go back to work, currently feeling somewhat depressed. Now,  he listens to a song which helps him to focus on his work. This changes his mood to a happy and exciting one. To do this all, all that Cuauthli needs to do is think about that one thing that he needs to do. The algorithms and his neurological link will take care of processing these thoughts and produce the sounds that he needs to hear. 

\section{Conclusion}
The visions and scenarios we presented come with their respective issues and challenges in implementation and in policy design. If we imagine a natural and seamless interface, evaluating its usability will introduce a new paradigm for HCI researchers. Will existing models such as Fitts' Law (which has always worked on any interface developed - mechanical, digital, virtual) still work in neurological links managed by our seamless thoughts? The intangible interaction provided by this \textit{"online network"} could potentially blur concepts such as piracy and intellectual property. As music is composed by ubiquitous algorithms connected to our personal thoughts and feelings, are all our emotions and the music that are generated by them considered unique? shareable? These among many others are interesting questions that we leave to our readers as we re-imagined a natural music interface. While these visions appear to be very far from reality, we are only left with our own thoughts to begin with and maybe hopefully in the not so near future too. 



%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
%%
%% End of file `sample-sigchi.tex'.
